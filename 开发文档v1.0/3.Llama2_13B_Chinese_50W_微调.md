> 参考：
>
> 1、微调：https://ukey.co/blog/finetune-llama-2-peft-qlora-huggingface/
>
> 2、微调：https://www.philschmid.de/fine-tune-flan-t5-peft
>
> 3、安装开发版transformer：https://huggingface.co/docs/transformers/installation
>
> 4、wandb无法初始化的问题：https://github.com/wandb/wandb/issues/5931

[toc]

## 先对数据盘进行扩容

- 如果当前机器无法扩容，那就需要找一个【同区】【有扩容空间的】机器，然后进行【克隆实例】
- 克隆完成后，把克隆机器无卡开机，然后对原机器进行【跨实例拷贝数据】，拷贝到克隆机器上
- 然后扩容克隆机器
- 后续都在克隆机器上进行操作即可

## 别省钱了，有卡开机

## 一、下载和预处理微调训练数据

1、我们使用BelleGroup提供的50w条中文数据，先下载到数据盘（cd autodl-tmp）：

```Bash
wget https://huggingface.co/datasets/BelleGroup/train_0.5M_CN/resolve/main/Belle_open_source_0.5M.json
```

原始数据的预览如下{"instruction":"xxxx", "input":"", "output":"xxxx"}：

![img](C:%5CUsers%5C98006%5CDesktop%5CLlama2%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3%5CLlama2_13B_Chinese_50W_%E5%BE%AE%E8%B0%83%EF%BC%88%E6%9C%AA%E5%BC%80%E5%A7%8B%EF%BC%89.assets%5Cimage-1691546309670.png)

2、新建一个文件：split_json.py，然后把下面的代码粘贴进去

```Bash
import random,json

def write_txt(file_path,datas):
    with open(file_path,"w",encoding="utf8") as f:
        for d in datas:
            f.write(json.dumps(d,ensure_ascii=False)+"\n")
        f.close()

with open("/root/autodl-tmp/Belle_open_source_0.5M.json","r",encoding="utf8") as f:
    lines=f.readlines()
    #拼接数据
    changed_data=[]
    for l in lines:
        l=json.loads(l)
        changed_data.append({"text":"### Human: "+l["instruction"]+" ### Assistant: "+l["output"]})

    #从拼好后的数据中，随机选出1000条，作为训练数据
    #为了省钱 和 演示使用，我们只用1000条，生产环境至少要使用全部50w条
    r_changed_data=random.sample(changed_data, 1000)

    #写到json中
    write_txt("/root/autodl-tmp/Belle_open_source_0.5M_changed_test.json",r_changed_data)
```

3、新建一个终端，执行下面的代码，把可以随机拆出1000条拼接好的数据给我们测试使用：

```Bash
python split_json.py
```

拼接好的数据格式如下图{"text":"### Human: xxxx ### Assistant: xxx"}：

![image](E:%5CDesktop%5CAPP%5CLlama2%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3%5C3.Llama2_13B_Chinese_50W_%E5%BE%AE%E8%B0%83%EF%BC%88%E6%9C%AA%E5%BC%80%E5%A7%8B%EF%BC%89.assets%5Cimage.png)

![image-1691546309670](E:%5CDesktop%5CAPP%5CLlama2%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3%5C3.Llama2_13B_Chinese_50W_%E5%BE%AE%E8%B0%83%EF%BC%88%E6%9C%AA%E5%BC%80%E5%A7%8B%EF%BC%89.assets%5Cimage-1691546309670.png)

## 二、运行微调文件

创建一个笔记本notebook。

1、首先要执行安装程序，使用-U强制升级到最新版本

```Bash
!pip install -q huggingface_hub
!pip install -q -U trl transformers accelerate peft
!pip install -q -U datasets bitsandbytes einops wandb
```

2、设置学术加速

```Bash
!source /etc/network_turbo
```

3、登录HF的notebook

需要到：https://huggingface.co/settings/tokens 复制token

然后执行下面的语句，并把token粘贴到输入框里进行登录

```Bash
from huggingface_hub import notebook_login
notebook_login()
```

4、初始化wandb

需要先到：https://wandb.me/wandb-server 注册wandb

然后到：https://wandb.ai/authorize 复制key出来

然后执行下面的语句后，把key输入到输入框里，回车即可

```Bash
import wandb
wandb.init()
```

5、导入相关包

```Bash
from datasets import load_dataset
import torch,einops
from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments
from peft import LoraConfig
from trl import SFTTrainer
```

6、加载上面拼接好之后的1000条数据

```Bash
dataset = load_dataset("json",data_files="/root/autodl-tmp/Belle_open_source_0.5M_changed_test.json",split="train")
```

7、配置本地模型

```Bash
base_model_name ="/root/autodl-tmp/Llama2-chat-13B-Chinese-50W"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,#在4bit上，进行量化
    bnb_4bit_use_double_quant=True,# 嵌套量化，每个参数可以多节省0.4位
    bnb_4bit_quant_type="nf4",#NF4（normalized float）或纯FP4量化 博客说推荐NF4
    bnb_4bit_compute_dtype=torch.float16,
)
```

8、配置GPU

```Bash
device_map = {"": 0}
#有多个gpu时，为：device_map = {"": [0,1,2,3……]}
```

9、加载本地模型

```Bash
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,#本地模型名称
    quantization_config=bnb_config,#上面本地模型的配置
    device_map=device_map,#使用GPU的编号
    trust_remote_code=True,
    use_auth_token=True
)
base_model.config.use_cache = False
base_model.config.pretraining_tp = 1
```

10、配置QLora

```Bash
#参数是啥意思我也不知道，大家会调库就行了
peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
```

11、对本地模型，把长文本拆成最小的单元词（即token）

```Bash
tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
```

12、训练参数的配置

```Bash
output_dir = "./results"
training_args = TrainingArguments(
    report_to="wandb",
    output_dir=output_dir,#训练后输出目录
    per_device_train_batch_size=4,#每个GPU的批处理数据量
    gradient_accumulation_steps=4,#在执行反向传播/更新过程之前，要累积其梯度的更新步骤数
    learning_rate=2e-4,#超参、初始学习率。太大模型不稳定，太小则模型不能收敛
    logging_steps=10,#两个日志记录之间的更新步骤数
    max_steps=100#要执行的训练步骤总数
)
max_seq_length = 512
#TrainingArguments 的参数详解：https://blog.csdn.net/qq_33293040/article/details/117376382

trainer = SFTTrainer(
    model=base_model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_args,
)
```

13、开始进行微调训练

```Bash
trainer.train()
```

14、把训练好的模型，保存下来

```Bash
import os
output_dir = os.path.join(output_dir, "final_checkpoint")
trainer.model.save_pretrained(output_dir)
```

## 三、执行代码合并（开卡运行）

1、新建一个merge_model.py的文件，把下面的代码粘贴进去：

```Bash
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

#设置原来本地模型的地址
model_name_or_path = '/root/autodl-tmp/Llama2-chat-13B-Chinese-50W'
#设置微调后模型的地址，就是上面的那个地址
adapter_name_or_path = '/root/autodl-tmp/results/final_checkpoint'
#设置合并后模型的导出地址
save_path = '/root/autodl-tmp/new_model'

tokenizer = AutoTokenizer.from_pretrained(
    model_name_or_path,
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    trust_remote_code=True,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16,
    device_map='auto'
)
print("load model success")
model = PeftModel.from_pretrained(model, adapter_name_or_path)
print("load adapter success")
model = model.merge_and_unload()
print("merge success")

tokenizer.save_pretrained(save_path)
model.save_pretrained(save_path)
print("save done.")
```

3、新建终端，然后执行上述合并代码，进行合并

```Bash
python merge_model.py
```

## 四、使用gradio运行模型

进入Llama2文件夹：cd Llama2

```Plain
python gradio_demo.py --base_model /root/autodl-tmp/new_model --tokenizer_path /root/autodl-tmp/new_model --gpus 0
```

