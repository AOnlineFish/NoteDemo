### 视频教程地址：

## 有卡开机

## 一、copy一份gradio_demo.py的文件

在部署的教程中（见：[Llama2—文字版部署教程](https://xtx0o8yn7x.feishu.cn/docx/ZdzYdNfHPoKkFMx5EBOcfjh0npb) ），我们有一份gradio_demo.py文件，copy一份到数据盘。

也可以直接下载：

gradio_demo.py

## 二、引入LangChain的相关包

```Python
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
```

## 三、修改setup()函数

- setup()函数，是gradio的初始化函数，在这个函数里会使用gradio加载前端页面、并加载本地模型。
- 这里，我们在setup()函数中，对【知识】进行chunk切分，并存储到本地向量库。如果本地已有向量库，就直接读取本地向量库。

```Python
#把 vector_store 设置为全局变量
global tokenizer, model, device, share, port, max_memory, vector_store
#先读取embedding模型
embeddings=HuggingFaceEmbeddings(model_name="/root/autodl-tmp/text2vec-large-chinese", model_kwargs={'device': 'cuda'})
#如果之前没有本地的faiss仓库，就把doc读取到向量库后，再把向量库保存到本地
if os.path.exists("/root/autodl-tmp/my_faiss_store")==False:
    #=======加载知识库=======
    filepath="knowledge.txt"
    loader=UnstructuredFileLoader(filepath)
    docs=loader.load()
    text_splitter=RecursiveCharacterTextSplitter(chunk_size=20,chunk_overlap=10)
    docs=text_splitter.split_documents(docs)
    
    vector_store=FAISS.from_documents(docs,embeddings)
    vector_store.save_local("/root/autodl-tmp/my_faiss_store")
#如果本地已经有faiss仓库了，说明之前已经保存过了，就直接读取
else:
    vector_store=FAISS.load_local("/root/autodl-tmp/my_faiss_store",embeddings=embeddings)
```

## 四、修改generate_prompt()函数

```Python
def generate_prompt(instruction,my_input):
    return f"已知：{my_input}\n请问：{instruction}"
```

## 五、修改predict()函数

```Python
    #history的格式：[[query1,response1],[query2,response2],[query3,response3]……]
    docs=vector_store.similarity_search(history[-1][0])
    context=[doc.page_content for doc in docs]
    #使用下面的方式，把多轮对话转为单轮对话
    input = f"### Instruction:{history[-1][0]} ### Response:{history[-1][1]}"
    prompt = generate_prompt(input,"".join(context))
```

## 六、执行gradio.py

```Python
python gradio_demo.py --base_model /root/autodl-tmp/Llama2-chat-13B-Chinese-50W --tokenizer_path /root/autodl-tmp/Llama2-chat-13B-Chinese-50W --gpus 0
```

<img src="E:%5CDesktop%5CAPP%5CLlama2%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3%5C5.%E5%88%86%E5%8F%91%E5%92%8C%E6%B5%8B%E8%AF%95.assets%5C1694415997750.png" alt="1694415997750" style="zoom:200%;" />