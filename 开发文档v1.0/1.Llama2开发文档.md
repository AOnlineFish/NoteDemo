## 为了省钱，无卡开机

可以使用nvidia-smi查看GPU内存

[toc]

## 环境配置

> **镜像**
>
> PyTorch 2.0.0
>
> Python 3.8(ubuntu20.04)
>
> Cuda 11.8
>
> **GPU**
>
> V100-32GB(32GB) * 1
>
> **CPU** 6 vCPU Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz
>
> **内存** 25GB
>
> **硬盘**
>
> 系统盘:30 GB
>
> 数据盘:免费:50GB SSD 付费:0GB扩容缩容
>
> **附加磁盘**
>
> 无
>
> **端口映射**
>
> 无
>
> **网络**同一地区实例共享带宽
>
> **计费方式**按量计费
>
> **费用**
>
> ￥1.88/时￥1.98/时

## 文件目录



> autodl-tmp/存放模型文件
>
> <img src="C:%5CUsers%5C98006%5CAppData%5CRoaming%5CTypora%5CdraftsRecover%5C2023-8-8%20%E4%B8%BA%E4%BA%86%E7%9C%81%E9%92%B1%EF%BC%8C%E6%97%A0%E5%8D%A1%E5%BC%80%E6%9C%BA%20170257.assets%5C1691545510184.png" alt="1691545510184"  />
>
> 新建文件夹Llama2/存放运行文件及所需安装包
>
> ![1691545669483](C:%5CUsers%5C98006%5CAppData%5CRoaming%5CTypora%5CdraftsRecover%5C2023-8-8%20%E4%B8%BA%E4%BA%86%E7%9C%81%E9%92%B1%EF%BC%8C%E6%97%A0%E5%8D%A1%E5%BC%80%E6%9C%BA%20170257.assets%5C1691545669483.png)



## 一、克隆模型仓库到数据盘

1、执行：git lfs install

执行之前，需要先安装git-lfs，使用下面两个命令进行安装：

```Plain
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
sudo apt-get install git-lfs
## 初始化lfs
git lfs install
```

（如果报错，就需要手动下载git-lfs，手动下载见这个教程：https://blog.csdn.net/anlian523/article/details/100520039）

2、克隆仓库到本地：

```Plain
git clone https://huggingface.co/RicardoLee/Llama2-chat-13B-Chinese-50W
```

#### 如果报错 则进行vpn代理：

> **设置学术加速**，不再区分不同地区
>
> ```
> source /etc/network_turbo
> ```
>
> **取消学术加速**，如果不再需要建议关闭学术加速，因为该加速可能对正常网络造成一定影响
>
> ```
> unset http_proxy && unset https_proxy
> ```

如果无法下载那3个模型文件，就需要手动下载；

手动下载需要：先进入到Llama2-chat-13B-Chinese-50W目录：cd Llama2-chat-13B-Chinese-50W

##### huggingface 地址： https://huggingface.co/RicardoLee/Llama2-chat-13B-Chinese-50W/tree/main

通过huggingface复制每个文件的下载链接，然后使用wget下载：

```Plain
# 应该会在50% 失败 对照文件发现 缺少模型文件使用wget 方法下载 
# 如果遇到ssl失效 加入参数 wget --no-check-certificate
wget --no-check-certificate https://huggingface.co/RicardoLee/Llama2-chat-13B-Chinese-50W/resolve/main/pytorch_model-00001-of-00003.bin
wget --no-check-certificate https://huggingface.co/RicardoLee/Llama2-chat-13B-Chinese-50W/resolve/main/pytorch_model-00002-of-00003.bin
wget --no-check-certificate https://huggingface.co/RicardoLee/Llama2-chat-13B-Chinese-50W/resolve/main/pytorch_model-00003-of-00003.bin
```

## 二、下载并部署gradio

1、把这个链接：https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/inference/gradio_demo.py 里的gradio_demo.py和requirements.txt下载到服务器

2、修改requrement.txt里的torch版本为2.0.0，然后安装requirements.txt

```Plain
pip install -r requirements.txt
```

3、把gradio.py里59、60、61行注释掉

然后手动安装gradio和gradio_demo.py里import的包；

安装gradio：

```Plain
pip install gradio -i http://pypi.douban.com/simple/  --trusted-host pypi.douban.com
```

安装bitsandbytes：

```Plain
pip install bitsandbytes
```

安装accelerate：

```Plain
pip install accelerate
```

安装scipy：

```Plain
pip install scipy
```

## 三、有卡开机，然后使用gradio运行模型

进入Llama2文件夹：cd Llama2

```Plain
python gradio_demo.py --base_model /root/autodl-tmp/Llama2-chat-13B-Chinese-50W --tokenizer_path /root/autodl-tmp/Llama2-chat-13B-Chinese-50W --gpus 0
```

## 注：好像要开vpn才能生成gradio的分享链接

<img src="C:%5CUsers%5C98006%5CAppData%5CRoaming%5CTypora%5CdraftsRecover%5C2023-8-8%20%E4%B8%BA%E4%BA%86%E7%9C%81%E9%92%B1%EF%BC%8C%E6%97%A0%E5%8D%A1%E5%BC%80%E6%9C%BA%20170257.assets%5C1691545301541.png" alt="1691545301541" style="zoom:200%;" />

跑完进度条后，会给出两个链接，采用第二个公共链接（具有时效性，大概时间24-48h）



