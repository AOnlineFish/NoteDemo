# 源码安装

本教程为您提供了关于如何使用DB-GPT的使用指南。

## 安装

请按照以下步骤安装DB-GPT

### 1. Hardware Requirements

由于我们的项目有能力达到85%以上的ChatGPT性能，所以对硬件有一定的要求。但总体来说，我们在消费级的显卡上即可完成项目的部署使用，具体部署的硬件说明如下:

| GPU      | 显存  | Performance                                         |
| -------- | ----- | --------------------------------------------------- |
| RTX 4090 | 24 GB | Smooth conversation inference                       |
| RTX 3090 | 24 GB | Smooth conversation inference, better than V100     |
| V100     | 16 GB | Conversation inference possible, noticeable stutter |
| T4       | 16 GB | Conversation inference possible, noticeable stutter |

如果你的显存不够，DB-GPT支持8-bit和4-bit量化版本

这里是量化版本的相关说明

| Model           | Quantize | 显存  |
| --------------- | -------- | ----- |
| vicuna-7b-v1.5  | 4-bit    | 8 GB  |
| vicuna-7b-v1.5  | 8-bit    | 12 GB |
| vicuna-13b-v1.5 | 4-bit    | 12 GB |
| vicuna-13b-v1.5 | 8-bit    | 20 GB |
| llama-2-7b      | 4-bit    | 8 GB  |
| llama-2-7b      | 8-bit    | 12 GB |
| llama-2-13b     | 4-bit    | 12 GB |
| llama-2-13b     | 8-bit    | 20 GB |
| llama-2-70b     | 4-bit    | 48 GB |
| llama-2-70b     | 8-bit    | 80 GB |
| baichuan-7b     | 4-bit    | 8 GB  |
| baichuan-7b     | 8-bit    | 12 GB |
| baichuan-13b    | 4-bit    | 12 GB |
| baichuan-13b    | 8-bit    | 20 GB |

### 2. Install

```
git clone https://github.com/eosphoros-ai/DB-GPT.git
```



目前使用Sqlite作为默认数据库，因此DB-GPT快速部署不需要部署相关数据库服务。如果你想使用其他数据库，需要先部署相关数据库服务。我们目前使用Miniconda进行python环境和包依赖管理[安装 Miniconda](https://docs.conda.io/en/latest/miniconda.html)

```
python>=3.10
conda create -n dbgpt_env python=3.10
conda activate dbgpt_env
pip install -e ".[default]"
```



在使用知识库之前

```
python -m spacy download zh_core_web_sm
```



如果你已经安装好了环境需要创建models, 然后到huggingface官网下载模型

小技巧

Notice make sure you have install git-lfs

centos:yum install git-lfs

ubuntu:app-get install git-lfs

macos:brew install git-lfs

```
cd DB-GPT
mkdir models and cd models
#### llm model
git clone https://huggingface.co/lmsys/vicuna-13b-v1.5
or
git clone https://huggingface.co/THUDM/chatglm2-6b

#### embedding model
git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese
or
git clone https://huggingface.co/moka-ai/m3e-large
```



模型文件很大，需要很长时间才能下载。在下载过程中，让我们配置.env文件，它需要从。env.template中复制和创建。

如果想使用openai大模型服务, 可以参考[LLM Use FAQ](https://db-gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)

小技巧

cp .env.template .env

您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。

您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5)， 目前Vicuna-v1.5模型(基于llama2)已经开源了，我们推荐你使用这个模型通过设置LLM_MODEL=vicuna-13b-v1.5

### 3. Run

**(Optional) load examples into SQLlite**

```
bash ./scripts/examples/load_examples.sh
```



On windows platform:

```
.\scripts\examples\load_examples.bat
```



1.Run db-gpt server

```
python pilot/server/dbgpt_server.py
```



打开浏览器访问http://localhost:5000

小技巧

If you want to access an external LLM service, you need to

1.set the variables LLM_MODEL=YOUR_MODEL_NAME, MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in the .env file.

2.execute dbgpt_server.py in light mode

如果你想了解web-ui, 请访问https://github./csunny/DB-GPT/tree/new-page-framework/datacenter

```
python pilot/server/dbgpt_server.py --light
```



### Multiple GPUs

DB-GPT默认加载可利用的gpu，你也可以通过修改 在`.env`文件 `CUDA_VISIBLE_DEVICES=0,1`来指定gpu IDs

你也可以指定gpu ID启动

```
# Specify 1 gpu
CUDA_VISIBLE_DEVICES=0 python3 pilot/server/dbgpt_server.py

# Specify 4 gpus
CUDA_VISIBLE_DEVICES=3,4,5,6 python3 pilot/server/dbgpt_server.py
```



同时你可以通过在.env文件设置`MAX_GPU_MEMORY=xxGib`修改每个GPU的最大使用内存

### Not Enough Memory

DB-GPT 支持 8-bit quantization 和 4-bit quantization.

你可以通过在.env文件设置`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True`

Llama-2-70b with 8-bit quantization 可以运行在80GB VRAM机器， 4-bit quantization可以运行在 48 GB VRAM