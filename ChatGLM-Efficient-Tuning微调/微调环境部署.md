[toc]

## [è½¯ä»¶ä¾èµ–](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md#è½¯ä»¶ä¾èµ–)

- Python 3.8+, PyTorch 1.13.1
- ğŸ¤—Transformers, Datasets, Accelerate, PEFT, TRL
- protobuf, cpm-kernels, sentencepiece
- jieba, rouge-chinese, nltkï¼ˆç”¨äºè¯„ä¼°ï¼‰
- gradio, matplotlibï¼ˆç”¨äºç½‘é¡µç«¯äº¤äº’ï¼‰
- uvicorn, fastapi, sse-starletteï¼ˆç”¨äº APIï¼‰

ä»¥åŠ **å¼ºè€Œæœ‰åŠ›çš„ GPU**ï¼

### [ç¯å¢ƒæ­å»ºï¼ˆå¯è·³è¿‡ï¼‰](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md#ç¯å¢ƒæ­å»ºå¯è·³è¿‡)

```bash
git lfs install
git clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git
conda create -n chatglm_etuning python=3.10
conda activate chatglm_etuning
cd ChatGLM-Efficient-Tuning
pip install -r requirements.txt
```



å¦‚æœè¦åœ¨ Windows å¹³å°ä¸Šå¼€å¯é‡åŒ– LoRAï¼ˆQLoRAï¼‰ï¼Œéœ€è¦å®‰è£…é¢„ç¼–è¯‘çš„ `bitsandbytes` åº“, æ”¯æŒ CUDA 11.1 åˆ° 12.1.

```bash
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl
```

### [å• GPU å¾®è°ƒè®­ç»ƒ](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md#å•-gpu-å¾®è°ƒè®­ç»ƒ)

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path /root/autodl-tmp/chatglm3-6b-base \
    --do_train \
    --dataset test_zh \
    --finetuning_type lora \
    --output_dir out \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 2 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-3 \
    --num_train_epochs 10.0 \
    --plot_loss \
    --fp16
```

### å• GPUå¾®è°ƒè®­ç»ƒ -ä¸ªäººé…ç½®

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path /root/autodl-tmp/chatglm-6b \
    --do_train \
    --dataset test_zh \
    --finetuning_type lora \
    --output_dir output_model \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 2 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-3 \
    --num_train_epochs 10.0 \
    --plot_loss \
    --fp16
```

### [å‘½ä»¤è¡Œæµ‹è¯•](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md#å‘½ä»¤è¡Œæµ‹è¯•)

```bash
CUDA_VISIBLE_DEVICES=0 python src/cli_demo.py \
    --model_name_or_path /root/autodl-tmp/chatglm-6b \
    --checkpoint_dir /root/autodl-tmp/ChatGLM-Efficient-Tuning/out
```

### [å¯¼å‡ºå¾®è°ƒæ¨¡å‹](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md#å¯¼å‡ºå¾®è°ƒæ¨¡å‹)

```
python src/export_model.py \
    --model_name_or_path  /root/autodl-tmp/chatglm-6b  \
    --finetuning_type lora \
    --checkpoint_dir /root/autodl-tmp/ChatGLM-Efficient-Tuning/output_model \
    --output_dir /root/autodl-tmp/ChatGLM-Efficient-Tuning/model_out
```

### FAQæ±‡æ€»

> å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†ï¼Ÿ

```bash

	å°†è‡ªå·±éœ€è¦è¿›è¡Œè®­ç»ƒçš„æ•°æ®é›†æ”¾åˆ°dataç›®å½•ä¸‹
	åœ¨dataset_info.json ä¸­è¿›è¡Œé…ç½®
	,
	"test_zh":{
		"file_name":"ä½ çš„æ•°æ®é›†"
	}
	
	,
	"test_zh":{
		"file_name":"generated_json_data_all.json"
	}
	
	ç„¶åå°†è®­ç»ƒå‚æ•°å˜æ›´ä¸º --dataset test_zh
```

