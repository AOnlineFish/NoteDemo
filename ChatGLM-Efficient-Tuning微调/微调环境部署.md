[toc]

## [软件依赖](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md#软件依赖)

- Python 3.8+, PyTorch 1.13.1
- 🤗Transformers, Datasets, Accelerate, PEFT, TRL
- protobuf, cpm-kernels, sentencepiece
- jieba, rouge-chinese, nltk（用于评估）
- gradio, matplotlib（用于网页端交互）
- uvicorn, fastapi, sse-starlette（用于 API）

以及 **强而有力的 GPU**！

### [环境搭建（可跳过）](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md#环境搭建可跳过)

```bash
git lfs install
git clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git
conda create -n chatglm_etuning python=3.10
conda activate chatglm_etuning
cd ChatGLM-Efficient-Tuning
pip install -r requirements.txt
```



如果要在 Windows 平台上开启量化 LoRA（QLoRA），需要安装预编译的 `bitsandbytes` 库, 支持 CUDA 11.1 到 12.1.

```bash
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl
```

### [单 GPU 微调训练](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md#单-gpu-微调训练)

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path /root/autodl-tmp/chatglm3-6b-base \
    --do_train \
    --dataset test_zh \
    --finetuning_type lora \
    --output_dir out \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 2 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-3 \
    --num_train_epochs 10.0 \
    --plot_loss \
    --fp16
```

### 单 GPU微调训练 -个人配置

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path /root/autodl-tmp/chatglm-6b \
    --do_train \
    --dataset test_zh \
    --finetuning_type lora \
    --output_dir output_model \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 2 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-3 \
    --num_train_epochs 10.0 \
    --plot_loss \
    --fp16
```

### [命令行测试](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md#命令行测试)

```bash
CUDA_VISIBLE_DEVICES=0 python src/cli_demo.py \
    --model_name_or_path /root/autodl-tmp/chatglm-6b \
    --checkpoint_dir /root/autodl-tmp/ChatGLM-Efficient-Tuning/out
```

### [导出微调模型](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md#导出微调模型)

```
python src/export_model.py \
    --model_name_or_path  /root/autodl-tmp/chatglm-6b  \
    --finetuning_type lora \
    --checkpoint_dir /root/autodl-tmp/ChatGLM-Efficient-Tuning/output_model \
    --output_dir /root/autodl-tmp/ChatGLM-Efficient-Tuning/model_out
```

### FAQ汇总

> 如何使用自定义数据集？

```bash

	将自己需要进行训练的数据集放到data目录下
	在dataset_info.json 中进行配置
	,
	"test_zh":{
		"file_name":"你的数据集"
	}
	
	,
	"test_zh":{
		"file_name":"generated_json_data_all.json"
	}
	
	然后将训练参数变更为 --dataset test_zh
```

